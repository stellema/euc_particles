{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from main import paths, im_ext, idx_1d, LAT_DEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data3/hh5/public/apps/miniconda3/envs/analysis3-19.07/lib/python3.6/site-packages/xarray/backends/api.py:783: FutureWarning: In xarray version 0.13 `auto_combine` will be deprecated.\n",
      "  coords=coords)\n",
      "/g/data3/hh5/public/apps/miniconda3/envs/analysis3-19.07/lib/python3.6/site-packages/xarray/backends/api.py:783: FutureWarning: The datasets supplied have global dimension coordinates. You may want\n",
      "to use the new `combine_by_coords` function (or the\n",
      "`combine='by_coords'` option to `open_mfdataset` to order the datasets\n",
      "before concatenation. Alternatively, to continue concatenating based\n",
      "on the order the datasets are supplied in in future, please use the\n",
      "new `combine_nested` function (or the `combine='nested'` option to\n",
      "open_mfdataset).\n",
      "  coords=coords)\n",
      "/g/data3/hh5/public/apps/miniconda3/envs/analysis3-19.07/lib/python3.6/site-packages/xarray/backends/api.py:783: FutureWarning: In xarray version 0.13 `auto_combine` will be deprecated.\n",
      "  coords=coords)\n",
      "/g/data3/hh5/public/apps/miniconda3/envs/analysis3-19.07/lib/python3.6/site-packages/xarray/backends/api.py:783: FutureWarning: The datasets supplied have global dimension coordinates. You may want\n",
      "to use the new `combine_by_coords` function (or the\n",
      "`combine='by_coords'` option to `open_mfdataset` to order the datasets\n",
      "before concatenation. Alternatively, to continue concatenating based\n",
      "on the order the datasets are supplied in in future, please use the\n",
      "new `combine_nested` function (or the `combine='nested'` option to\n",
      "open_mfdataset).\n",
      "  coords=coords)\n",
      "/g/data3/hh5/public/apps/miniconda3/envs/analysis3-19.07/lib/python3.6/site-packages/xarray/backends/api.py:783: FutureWarning: In xarray version 0.13 `auto_combine` will be deprecated.\n",
      "  coords=coords)\n",
      "/g/data3/hh5/public/apps/miniconda3/envs/analysis3-19.07/lib/python3.6/site-packages/xarray/backends/api.py:783: FutureWarning: The datasets supplied have global dimension coordinates. You may want\n",
      "to use the new `combine_by_coords` function (or the\n",
      "`combine='by_coords'` option to `open_mfdataset` to order the datasets\n",
      "before concatenation. Alternatively, to continue concatenating based\n",
      "on the order the datasets are supplied in in future, please use the\n",
      "new `combine_nested` function (or the `combine='nested'` option to\n",
      "open_mfdataset).\n",
      "  coords=coords)\n",
      "/g/data3/hh5/public/apps/miniconda3/envs/analysis3-19.07/lib/python3.6/site-packages/xarray/backends/api.py:783: FutureWarning: In xarray version 0.13 `auto_combine` will be deprecated.\n",
      "  coords=coords)\n",
      "/g/data3/hh5/public/apps/miniconda3/envs/analysis3-19.07/lib/python3.6/site-packages/xarray/backends/api.py:783: FutureWarning: The datasets supplied have global dimension coordinates. You may want\n",
      "to use the new `combine_by_coords` function (or the\n",
      "`combine='by_coords'` option to `open_mfdataset` to order the datasets\n",
      "before concatenation. Alternatively, to continue concatenating based\n",
      "on the order the datasets are supplied in in future, please use the\n",
      "new `combine_nested` function (or the `combine='nested'` option to\n",
      "open_mfdataset).\n",
      "  coords=coords)\n"
     ]
    }
   ],
   "source": [
    "# Path to save figures, save data and OFAM model output.\n",
    "fpath, dpath, xpath = paths()\n",
    "\n",
    "ufiles = []\n",
    "vfiles = []\n",
    "sfiles = []\n",
    "tfiles = []\n",
    "year = [1981, 1981]\n",
    "# year = [2070, 2101]\n",
    "for y in range(year[0], year[-1] + 1):\n",
    "    for i, m in enumerate(range(1, 13)):\n",
    "        ufiles.append(xpath.joinpath('ocean_u_{}_{:02d}.nc'.format(y, m)))\n",
    "        vfiles.append(xpath.joinpath('ocean_v_{}_{:02d}.nc'.format(y, m)))\n",
    "        sfiles.append(xpath.joinpath('ocean_salt_{}_{:02d}.nc'.format(y, m)))\n",
    "        tfiles.append(xpath.joinpath('ocean_temp_{}_{:02d}.nc'.format(y, m)))\n",
    "du = xr.open_mfdataset(ufiles).groupby('Time.month').mean('Time')\n",
    "dv = xr.open_mfdataset(vfiles).groupby('Time.month').mean('Time')\n",
    "ds = xr.open_mfdataset(sfiles).groupby('Time.month').mean('Time')\n",
    "dt = xr.open_mfdataset(tfiles).groupby('Time.month').mean('Time')\n",
    "\n",
    "# Create copy of particle file with initally westward partciles removed.\n",
    "mask = xr.Dataset()\n",
    "mask['nv'] = du.nv\n",
    "mask['st_edges_ocean'] = du.st_edges_ocean\n",
    "mask['st_ocean'] = du.st_ocean\n",
    "mask['xu_ocean'] = du.xu_ocean\n",
    "mask['yu_ocean'] = du.yu_ocean\n",
    "mask['xt_ocean'] = ds.xt_ocean\n",
    "mask['yt_ocean'] = ds.yt_ocean\n",
    "mask.to_netcdf(dpath.joinpath('ocean_mesh_mask.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:         (month: 12, nv: 2, st_edges_ocean: 52, st_ocean: 51, xu_ocean: 1750, yu_ocean: 300)\n",
      "Coordinates:\n",
      "  * nv              (nv) float64 1.0 2.0\n",
      "  * st_edges_ocean  (st_edges_ocean) float64 0.0 5.0 10.0 ... 4.056e+03 5e+03\n",
      "  * st_ocean        (st_ocean) float64 2.5 7.5 12.5 ... 3.603e+03 4.509e+03\n",
      "  * xu_ocean        (xu_ocean) float64 120.0 120.1 120.2 ... 294.7 294.8 294.9\n",
      "  * yu_ocean        (yu_ocean) float64 -15.0 -14.9 -14.8 ... 14.7 14.8 14.9\n",
      "  * month           (month) int64 1 2 3 4 5 6 7 8 9 10 11 12\n",
      "Data variables:\n",
      "    Time_bounds     (month, nv) timedelta64[ns] dask.array<shape=(12, 2), chunksize=(1, 2)>\n",
      "    average_DT      (month) timedelta64[ns] dask.array<shape=(12,), chunksize=(1,)>\n",
      "    u               (month, st_ocean, yu_ocean, xu_ocean) float32 dask.array<shape=(12, 51, 300, 1750), chunksize=(1, 51, 300, 1750)>\n",
      "<xarray.Dataset>\n",
      "Dimensions:         (month: 12, nv: 2, st_edges_ocean: 52, st_ocean: 51, xu_ocean: 1750, yu_ocean: 300)\n",
      "Coordinates:\n",
      "  * nv              (nv) float64 1.0 2.0\n",
      "  * st_edges_ocean  (st_edges_ocean) float64 0.0 5.0 10.0 ... 4.056e+03 5e+03\n",
      "  * st_ocean        (st_ocean) float64 2.5 7.5 12.5 ... 3.603e+03 4.509e+03\n",
      "  * xu_ocean        (xu_ocean) float64 120.0 120.1 120.2 ... 294.7 294.8 294.9\n",
      "  * yu_ocean        (yu_ocean) float64 -15.0 -14.9 -14.8 ... 14.7 14.8 14.9\n",
      "  * month           (month) int64 1 2 3 4 5 6 7 8 9 10 11 12\n",
      "Data variables:\n",
      "    Time_bounds     (month, nv) timedelta64[ns] dask.array<shape=(12, 2), chunksize=(1, 2)>\n",
      "    average_DT      (month) timedelta64[ns] dask.array<shape=(12,), chunksize=(1,)>\n",
      "    v               (month, st_ocean, yu_ocean, xu_ocean) float32 dask.array<shape=(12, 51, 300, 1750), chunksize=(1, 51, 300, 1750)>\n",
      "<xarray.Dataset>\n",
      "Dimensions:         (month: 12, nv: 2, st_edges_ocean: 52, st_ocean: 51, xt_ocean: 1750, yt_ocean: 300)\n",
      "Coordinates:\n",
      "  * nv              (nv) float64 1.0 2.0\n",
      "  * st_edges_ocean  (st_edges_ocean) float64 0.0 5.0 10.0 ... 4.056e+03 5e+03\n",
      "  * st_ocean        (st_ocean) float64 2.5 7.5 12.5 ... 3.603e+03 4.509e+03\n",
      "  * xt_ocean        (xt_ocean) float64 120.1 120.2 120.2 ... 294.8 294.9 295.0\n",
      "  * yt_ocean        (yt_ocean) float64 -14.95 -14.85 -14.75 ... 14.85 14.95\n",
      "  * month           (month) int64 1 2 3 4 5 6 7 8 9 10 11 12\n",
      "Data variables:\n",
      "    Time_bounds     (month, nv) timedelta64[ns] dask.array<shape=(12, 2), chunksize=(1, 2)>\n",
      "    average_DT      (month) timedelta64[ns] dask.array<shape=(12,), chunksize=(1,)>\n",
      "    salt            (month, st_ocean, yt_ocean, xt_ocean) float32 dask.array<shape=(12, 51, 300, 1750), chunksize=(1, 51, 300, 1750)>\n",
      "<xarray.Dataset>\n",
      "Dimensions:         (month: 12, nv: 2, st_edges_ocean: 52, st_ocean: 51, xt_ocean: 1750, yt_ocean: 300)\n",
      "Coordinates:\n",
      "  * nv              (nv) float64 1.0 2.0\n",
      "  * st_edges_ocean  (st_edges_ocean) float64 0.0 5.0 10.0 ... 4.056e+03 5e+03\n",
      "  * st_ocean        (st_ocean) float64 2.5 7.5 12.5 ... 3.603e+03 4.509e+03\n",
      "  * xt_ocean        (xt_ocean) float64 120.1 120.2 120.2 ... 294.8 294.9 295.0\n",
      "  * yt_ocean        (yt_ocean) float64 -14.95 -14.85 -14.75 ... 14.85 14.95\n",
      "  * month           (month) int64 1 2 3 4 5 6 7 8 9 10 11 12\n",
      "Data variables:\n",
      "    Time_bounds     (month, nv) timedelta64[ns] dask.array<shape=(12, 2), chunksize=(1, 2)>\n",
      "    average_DT      (month) timedelta64[ns] dask.array<shape=(12,), chunksize=(1,)>\n",
      "    temp            (month, st_ocean, yt_ocean, xt_ocean) float32 dask.array<shape=(12, 51, 300, 1750), chunksize=(1, 51, 300, 1750)>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d7be519bab5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/g/data3/hh5/public/apps/miniconda3/envs/analysis3-19.07/lib/python3.6/site-packages/xarray/core/common.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    181\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         raise AttributeError(\"%r object has no attribute %r\" %\n\u001b[0;32m--> 183\u001b[0;31m                              (type(self).__name__, name))\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "ds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds\n",
    "# 143E, 156E, 165E, 170E, 180E, 170W, 155W, 140W, 125W, 110W, 95W\n",
    "jl = [143, 156, 165, 170, 180, 190, 205, 220, 235, 250, 265]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lons = [180]\n",
    "lats = [-0.5, 0.5]\n",
    "depths = [25, 50]\n",
    "ilats = [idx_1d(ds.yu_ocean.values, lats[0]), idx_1d(ds.yu_ocean.values, lats[-1])]\n",
    "ilons = [idx_1d(ds.xu_ocean.values, lons[0]), idx_1d(ds.xu_ocean.values, lons[-1])]\n",
    "idepths = [idx_1d(ds.st_ocean.values, depths[0]), idx_1d(ds.st_ocean.values, depths[-1])]\n",
    "ilats\n",
    "ilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.zeros((ds.u.shape))\n",
    "dk = [ds.st_edges_ocean[i+1].item() - ds.st_edges_ocean[i].item() for i in range(len(ds.st_edges_ocean) -1)]\n",
    "dy = 0.1 * LAT_DEG\n",
    "for i in np.arange(ilons[0], ilons[-1] + 1):\n",
    "    for j in np.arange(ilats[0], ilats[-1] + 1):\n",
    "        for t in range(12):\n",
    "            for k in np.arange(idepths[0], idepths[1] + 1):\n",
    "                # Velocity x Depth x Width.\n",
    "                u[t, k, j, i] = ds.u[t, k, j, i]*dk[k]*dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u[:, idepths[0], ilats[0], ilons[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
